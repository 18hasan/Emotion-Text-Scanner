{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjtibVoKi_Cu",
        "outputId": "641bb058-ad66-4631-d99a-be7259ac429b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "##DATA PREPROCESSING\n",
        "##NECESSARY LIBRARIES\n",
        "\n",
        "import pandas as pd  ## loading data  like CSV files\n",
        "import numpy as np ## numerical operations\n",
        "import re, nltk  ## Regular expressions are used for pattern matching\n",
        "\n",
        "from nltk.corpus import stopwords  ## stopwords are filtered out as they do not conatin any emotion stopwords are( the , is ,in)\n",
        "from nltk.tokenize import word_tokenize  ## text into individual words\n",
        "from nltk.stem import WordNetLemmatizer  ## to reduce words to their base just like for running it will be run\n",
        "from sklearn.preprocessing import LabelEncoder ##encode categorical labels into numerical l abels\n",
        "from sklearn.metrics import accuracy_score, classification_report ##encode categorical labels into numerical labels\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "##removes no not and nor from the set as they carry sentiment meaning\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.remove(\"no\")\n",
        "stop_words.remove(\"not\")\n",
        "stop_words.remove(\"nor\")\n",
        "stop_words = [x.lower() for x in stop_words] ##all stopwords are converted to lowercase\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## this both libraries are used for handling the imbalaneced data and to check performance and efficiency of algorithms\n",
        "\n",
        "!pip install lightgbm\n",
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89QNUCrbjUdI",
        "outputId": "de76065a-0150-4c21-d75f-42486a16b2c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.11.4)\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.3-cp310-cp310-manylinux2014_x86_64.whl (98.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## PREDEFINED WORDS\n",
        "\n",
        "contractions = {\n",
        "        \"ai\\snot\": \"am not\",\n",
        "        \"wo\\snot\": \"will not\",\n",
        "        \"would\\snot\": \"would not\",\n",
        "        \"should\\snot\": \"should not\",\n",
        "        \"isn\\snot\": \"is not\",\n",
        "        \"aren\\snot\": \"are not\",\n",
        "        \"wasn\\snot\": \"was not\",\n",
        "        \"weren\\snot\": \"were not\",\n",
        "        \"haven\\snot\": \"have not\",\n",
        "        \"hasn\\snot\": \"has not\",\n",
        "        \"hadn\\snot\": \"had not\",\n",
        "        \"don\\snot\": \"do not\",\n",
        "        \"doesn\\snot\": \"does not\",\n",
        "        \"didn\\snot\": \"did not\",\n",
        "        \"can\\snot\": \"can not\",\n",
        "        \"cannot\": \"can not\",\n",
        "        \"couldn\\snot\": \"could not\",\n",
        "        \"shouldn\\snot\": \"should not\",\n",
        "        \"mightn\\snot\": \"might not\",\n",
        "        \"mustn\\snot\": \"must not\",\n",
        "        \"shan\\snot\": \"shall not\",\n",
        "        \"won\\snot\": \"will not\",\n",
        "        \"ain\\snot\": \"am not\",\n",
        "        \"I\\sam\": \"I am\",\n",
        "        \"you\\sare\": \"you are\",\n",
        "        \"he\\sis\": \"he is\",\n",
        "        \"she\\sis\": \"she is\",\n",
        "        \"it\\sis\": \"it is\",\n",
        "        \"we\\sare\": \"we are\",\n",
        "        \"they\\sare\": \"they are\",\n",
        "        \"I\\swould\": \"I would\",\n",
        "        \"you\\swould\": \"you would\",\n",
        "        \"he\\swould\": \"he would\",\n",
        "        \"she\\swould\": \"she would\",\n",
        "        \"it\\swould\": \"it would\",\n",
        "        \"we\\swould\": \"we would\",\n",
        "        \"they\\swould\": \"they would\",\n",
        "        \"I\\scould\": \"I could\",\n",
        "        \"you\\scould\": \"you could\",\n",
        "        \"he\\scould\": \"he could\",\n",
        "        \"she\\scould\": \"she could\",\n",
        "        \"it\\scould\": \"it could\",\n",
        "        \"we\\scould\": \"we could\",\n",
        "        \"they\\scould\": \"they could\",\n",
        "        \"I\\shave\": \"I have\",\n",
        "        \"you\\shave\": \"you have\",\n",
        "        \"he\\shas\": \"he has\",\n",
        "        \"she\\shas\": \"she has\",\n",
        "        \"it\\shas\": \"it has\",\n",
        "        \"we\\shave\": \"we have\",\n",
        "        \"they\\shave\": \"they have\",\n",
        "        \"I\\swill\": \"I will\",\n",
        "        \"you\\swill\": \"you will\",\n",
        "        \"he\\swill\": \"he will\",\n",
        "        \"she\\swill\": \"she will\",\n",
        "        \"it\\swill\": \"it will\",\n",
        "        \"we\\swill\": \"we will\",\n",
        "        \"they\\swill\": \"they will\",\n",
        "        \"I\\smust\": \"I must\",\n",
        "        \"you\\smust\": \"you must\",\n",
        "        \"he\\smust\": \"he must\",\n",
        "        \"she\\smust\": \"she must\",\n",
        "        \"it\\smust\": \"it must\",\n",
        "        \"we\\smust\": \"we must\",\n",
        "        \"they\\smust\": \"they must\",\n",
        "        \"I\\sshall\": \"I shall\",\n",
        "        \"you\\sshall\": \"you shall\",\n",
        "        \"he\\sshall\": \"he shall\",\n",
        "        \"she\\sshall\": \"she shall\",\n",
        "        \"it\\sshall\": \"it shall\",\n",
        "        \"we\\sshall\": \"we shall\",\n",
        "        \"they\\sshall\": \"they shall\",\n",
        "        \"haven\\s't\": \"have not\",\n",
        "        \"hasn\\s't\": \"has not\",\n",
        "        \"hadn\\s't\": \"had not\",\n",
        "        \"don\\s't\": \"do not\",\n",
        "        \"doesn\\s't\": \"does not\",\n",
        "        \"didn\\s't\": \"did not\",\n",
        "        \"can\\s't\": \"can not\",\n",
        "        \"cannot\": \"can not\",\n",
        "        \"couldn\\s't\": \"could not\",\n",
        "        \"shouldn\\s't\": \"should not\",\n",
        "        \"mightn\\s't\": \"might not\",\n",
        "        \"mustn\\s't\": \"must not\",\n",
        "        \"shan\\s't\": \"shall not\",\n",
        "        \"won\\s't\": \"will not\",\n",
        "        \"ain\\s't\": \"am not\",\n",
        "        \"aren\\s't\": \"are not\",\n",
        "        \"wasn\\s't\": \"was not\",\n",
        "        \"weren\\s't\": \"were not\",\n",
        "        \"I\\sdidn't\": \"I did not\",\n",
        "        \"you\\sdidn't\": \"you did not\",\n",
        "        \"he\\sdidn't\": \"he did not\",\n",
        "        \"she\\sdidn't\": \"she did not\",\n",
        "        \"it\\sdidn't\": \"it did not\",\n",
        "        \"we\\sdidn't\": \"we did not\",\n",
        "        \"they\\sdidn't\": \"they did not\",\n",
        "        \"I\\scannot\": \"I can not\",\n",
        "        \"you\\scannot\": \"you can not\",\n",
        "        \"he\\scannot\": \"he can not\",\n",
        "        \"she\\scannot\": \"she can not\",\n",
        "        \"it\\scannot\": \"it can not\",\n",
        "        \"we\\scannot\": \"we can not\",\n",
        "        \"they\\scannot\": \"they can not\",\n",
        "        \"I\\swon't\": \"I will not\",\n",
        "        \"you\\swon't\": \"you will not\",\n",
        "        \"he\\swon't\": \"he will not\",\n",
        "        \"she\\swon't\": \"she will not\",\n",
        "        \"it\\swon't\": \"it will not\",\n",
        "        \"we\\swon't\": \"we will not\",\n",
        "        \"they\\swon't\": \"they will not\",\n",
        "        \"I\\shasn't\": \"I has not\",\n",
        "        \"you\\shasn't\": \"you has not\",\n",
        "        \"he\\shasn't\": \"he has not\",\n",
        "        \"she\\shasn't\": \"she has not\",\n",
        "        \"it\\shasn't\": \"it has not\",\n",
        "        \"we\\shasn't\": \"we has not\",\n",
        "        \"they\\shasn't\": \"they has not\"\n",
        "}\n",
        "\n",
        "nots = {\n",
        "    'not sad': 'Happy', 'not bad': 'Happy', 'not boring': 'Happy', 'not wrong': 'Happy', 'not bored': 'Happy',\n",
        "        'not jealous': 'Happy', 'not happy': 'Sad', 'not well': 'Sad', 'not suitable': 'Angry',\n",
        "        'not right': 'Angry', 'not good': 'Sad', 'not excited': 'Angry', 'not funny ': 'Sad', 'not kind': 'Sad',\n",
        "        'not proud': 'Angry', 'not cool': 'Angry', 'not funny': 'Angry', 'not kind': 'Angry', 'not open': 'Angry',\n",
        "        'not safe': 'Fear', 'not enough': 'Empty', 'not know': 'Sad', 'not knowing': 'Sad', 'not believe': 'Angry',\n",
        "        'not believing': 'Angry', 'not understand': 'Sad', 'not understanding': 'Sad', 'no doubt': 'Happy',\n",
        "        'not think': 'Sad', 'not thinking': 'Sad', 'not recognise': 'Sad', 'not recognising': 'Sad',\n",
        "        'not forget': 'Angry', 'not forgetting': 'Angry', 'not remember': 'Sad', 'not remembering': 'Sad',\n",
        "        'not imagine': 'Sad', 'not imagining': 'Sad', 'not mean': 'Sad', 'not meaning': 'Sad',\n",
        "        'not agree': 'Angry', 'not agreeing': 'Sad', 'not disagree': 'Happy', 'not disagreeing': 'Happy',\n",
        "        'not deny': 'Sad', 'not denying': 'Sad', 'not promise': 'Angry', 'not promising': 'Angry',\n",
        "        'not satisfy': 'Sad', 'not satisfying': 'Sad', 'not realise': 'Sad', 'not realising': 'Sad',\n",
        "        'not appear': 'Angry', 'not appearing': 'Angry', 'not please': 'Sad', 'not pleasing': 'Sad',\n",
        "        'not impress': 'Sad', 'not impressing': 'Sad', 'not surprise': 'Sad', 'not surprising': 'Sad',\n",
        "        'not concern': 'Sad', 'not concerning': 'Sad', 'not have': 'Sad', 'not having': 'Sad',\n",
        "        'not own': 'Sad', 'not owning': 'Sad', 'not possess': 'Sad', 'not possessing': 'Sad',\n",
        "        'not lack': 'Sad', 'not lacking': 'Sad', 'not consist': 'Sad', 'not consisting': 'Sad',\n",
        "        'not involve': 'Sad', 'not involving': 'Sad', 'not include': 'Sad', 'not including': 'Sad',\n",
        "        'not contain': 'Sad', 'not containing': 'Sad', 'not love': 'Sad', 'not like': 'Angry',\n",
        "        'not hate': 'Happy', 'not hating': 'Happy', 'not adore': 'Sad', 'not adoring': 'Sad',\n",
        "        'not prefer': 'Sad', 'not preferring': 'Sad', 'not care': 'Angry', 'not mind': 'Angry',\n",
        "        'not minding': 'Sad', 'not want': 'Angry', 'not wanting': 'Sad', 'not need': 'Angry',\n",
        "        'not needing': 'Angry', 'not desire': 'Sad', 'not desiring': 'Sad', 'not wish': 'Sad',\n",
        "        'not wishing': 'Sad', 'not hope': 'Sad', 'not hoping': 'Sad', 'not appreciate': 'Sad',\n",
        "        'not appreciating': 'Sad', 'not value': 'Sad', 'not valuing': 'Sad', 'not owe': 'Sad',\n",
        "        'not owing': 'Sad', 'not seem': 'Sad', 'not seeming': 'Sad', 'not fit': 'Sad', 'not fitting': 'Sad',\n",
        "        'not depend': 'Sad', 'not depending': 'Sad', 'not matter': 'Sad', 'not afford': 'Sad',\n",
        "        'not affording': 'Sad', 'not aim': 'Sad', 'not aiming': 'Sad', 'not attempt': 'Angry',\n",
        "        'not attempting': 'Angry', 'not ask': 'Angry', 'not asking': 'Angry', 'not arrange': 'Angry',\n",
        "        'not arranging': 'Angry', 'not beg': 'Angry', 'not begging': 'Angry', 'not begin': 'Angry',\n",
        "        'not beginning': 'Angry', 'not caring': 'Angry', 'not choose': 'Angry', 'not choosing': 'Angry',\n",
        "        'not claim': 'Angry', 'not claiming': 'Angry', 'not consent': 'Angry', 'not consenting': 'Angry',\n",
        "        'not continue': 'Angry', 'not continuing': 'Angry', 'not dare': 'Angry', 'not daring': 'Angry',\n",
        "        'not decide': 'Sad', 'not deciding': 'Sad', 'not demand': 'Angry', 'not demanding': 'Angry',\n",
        "        'not deserve': 'Angry', 'not deserving': 'Angry', 'not expect': 'Angry', 'not expecting': 'Angry',\n",
        "        'not fail': 'Happy', 'not failing': 'Happy', 'not get': 'Sad', 'not getting': 'Sad',\n",
        "        'not hesitate': 'Sad', 'not hesitating': 'Sad', 'not hurry': 'Happy', 'not hurrying': 'Happy',\n",
        "        'not intend': 'Sad', 'not intending': 'Sad', 'not learn': 'Angry', 'not learning': 'Angry',\n",
        "        'not liking': 'Angry', 'not loving': 'Sad', 'not manage': 'Angry', 'not managing': 'Angry',\n",
        "        'not neglect': 'Sad', 'not neglecting': 'Sad', 'not offer': 'Angry', 'not offering': 'Angry',\n",
        "        'not plan': 'Angry', 'not planing': 'Angry', 'not prepare': 'Angry', 'not preparing': 'Angry',\n",
        "        'not pretend': 'Angry', 'not pretending': 'Angry', 'not proceed': 'Angry', 'not proceeding': 'Angry',\n",
        "        'not propose': 'Angry', 'not proposing': 'Sad', 'not refuse': 'Sad', 'not refusing': 'Sad',\n",
        "        'not start': 'Sad', 'not starting': 'Sad', 'not stop': 'Happy', 'not stopping': 'Happy',\n",
        "        'not struggle': 'Angry', 'not struggling': 'Angry', 'not swear': 'Angry', 'not swearing': 'Angry',\n",
        "        'not threaten': 'Happy', 'not threatening': 'Happy', 'not try': 'Angry', 'not trying': 'Angry',\n",
        "        'not volunteer': 'Angry', 'not volunteering': 'Angry', 'not wait': 'Angry', 'not waiting': 'Angry',\n",
        "        'not feel': 'Sad', 'not feeling': 'Sad', \"not able\": \"Sad\", \"not do\": \"Sad\",\n",
        "        'not apologize': 'Sad', 'not apologizing': 'Sad', 'not forgive': 'Angry', 'not forgiving': 'Angry',\n",
        "        'not trust': 'Angry', 'not trusting': 'Angry', 'not regret': 'Angry', 'not regretting': 'Angry',\n",
        "        'not rejoice': 'Sad', 'not rejoicing': 'Sad', 'not admire': 'Sad', 'not admiring': 'Sad',\n",
        "        'not compliment': 'Sad', 'not complimenting': 'Sad', 'not criticize': 'Happy', 'not criticizing': 'Happy',\n",
        "        'not encourage': 'Angry', 'not encouraging': 'Angry', 'not insult': 'Sad', 'not insulting': 'Sad',\n",
        "        'not praise': 'Angry', 'not praising': 'Angry', 'not support': 'Angry', 'not supporting': 'Angry',\n",
        "        'not blame': 'Sad', 'not blaming': 'Sad', 'not defend': 'Sad', 'not defending': 'Sad',\n",
        "        'not appreciate': 'Sad', 'not appreciating': 'Sad', 'not enjoy': 'Sad', 'not enjoying': 'Sad',\n",
        "        'not like': 'Angry', 'not liking': 'Angry', 'not love': 'Sad', 'not loving': 'Sad',\n",
        "        'not prefer': 'Sad', 'not preferring': 'Sad', 'not want': 'Angry', 'not wanting': 'Sad',\n",
        "        'not believe': 'Angry', 'not believing': 'Angry', 'not doubt': 'Happy', 'not doubting': 'Happy',\n",
        "        'not imagine': 'Sad', 'not imagining': 'Sad', 'not realize': 'Sad', 'not realizing': 'Sad',\n",
        "        'not remember': 'Sad', 'not remembering': 'Sad', 'not recognize': 'Sad', 'not recognizing': 'Sad',\n",
        "        'not consider': 'Sad', 'not considering': 'Sad', 'not think': 'Sad', 'not thinking': 'Sad',\n",
        "        'not forget': 'Angry', 'not forgetting': 'Angry', 'not ignore': 'Angry', 'not ignoring': 'Angry',\n",
        "        'not overlook': 'Angry', 'not overlooking': 'Angry', 'not understand': 'Sad', 'not understanding': 'Sad',\n",
        "        'not hear': 'Angry', 'not hearing': 'Angry', 'not listen': 'Angry', 'not listening': 'Angry',\n",
        "        'not look': 'Angry', 'not looking': 'Angry', 'not smell': 'Angry', 'not smelling': 'Angry',\n",
        "        'not taste': 'Angry', 'not tasting': 'Angry', 'not touch': 'Angry', 'not touching': 'Angry',\n",
        "        'not feel': 'Sad', 'not feeling': 'Sad', 'not sense': 'Sad', 'not sensing': 'Sad',\n",
        "        'not suppose': 'Angry', 'not supposing': 'Angry', 'not expect': 'Angry', 'not expecting': 'Angry',\n",
        "        'not wait': 'Angry', 'not waiting': 'Angry', 'not long': 'Angry', 'not longing': 'Angry',\n",
        "        'not yearn': 'Angry', 'not yearning': 'Angry', 'not wish': 'Sad', 'not wishing': 'Sad',\n",
        "        'not hope': 'Sad', 'not hoping': 'Sad', 'not desire': 'Sad', 'not desiring': 'Sad',\n",
        "        'not miss': 'Angry', 'not missing': 'Angry', 'not need': 'Angry', 'not needing': 'Angry',\n",
        "        'not want': 'Angry', 'not wanting': 'Sad', 'not require': 'Angry', 'not requiring': 'Angry',\n",
        "        'not demand': 'Angry', 'not demanding': 'Angry', 'not insist': 'Angry', 'not insisting': 'Angry',\n",
        "        'not force': 'Angry', 'not forcing': 'Angry', 'not push': 'Angry', 'not pushing': 'Angry',\n",
        "        'not pull': 'Angry', 'not pulling': 'Angry', 'not drag': 'Angry', 'not dragging': 'Angry',\n",
        "        'not carry': 'Angry', 'not carrying': 'Angry', 'not lift': 'Angry', 'not lifting': 'Angry',\n",
        "        'not drop': 'Angry', 'not dropping': 'Angry', 'not throw': 'Angry', 'not throwing': 'Angry',\n",
        "        'not catch': 'Angry', 'not catching': 'Angry', 'not capture': 'Angry', 'not capturing': 'Angry',\n",
        "        'not grab': 'Angry', 'not grabbing': 'Angry', 'not touch': 'Angry', 'not touching': 'Angry',\n",
        "        'not reach': 'Angry', 'not reaching': 'Angry', 'not approach': 'Angry', 'not approaching': 'Angry',\n",
        "        'not avoid': 'Angry', 'not avoiding': 'Angry', 'not evade': 'Angry', 'not evading': 'Angry',\n",
        "        'not elude': 'Angry', 'not eluding': 'Angry', 'not escape': 'Angry', 'not escaping': 'Angry',\n",
        "        'not run': 'Angry', 'not running': 'Angry', 'not jog': 'Angry', 'not jogging': 'Angry',\n",
        "        'not walk': 'Angry', 'not walking': 'Angry', 'not crawl': 'Angry', 'not crawling': 'Angry',\n",
        "        'not sneak': 'Angry', 'not sneaking': 'Angry', 'not tiptoe': 'Angry', 'not tiptoeing': 'Angry',\n",
        "        'not dance': 'Angry', 'not dancing': 'Angry', 'not stomp': 'Angry', 'not stomping': 'Angry',\n",
        "        'not shake': 'Angry', 'not shaking': 'Angry', 'not tremble': 'Angry', 'not trembling': 'Angry',\n",
        "        'not shiver': 'Angry', 'not shivering': 'Angry', 'not quiver': 'Angry', 'not quivering': 'Angry',\n",
        "        'not vibrate': 'Angry', 'not vibrating': 'Angry', 'not pulsate': 'Angry', 'not pulsating': 'Angry',\n",
        "        'not throb': 'Angry', 'not throbbing': 'Angry', 'not beat': 'Angry', 'not beating': 'Angry',\n",
        "        'not palpitate': 'Angry', 'not palpitating': 'Angry', 'not pump': 'Angry', 'not pumping': 'Angry',\n",
        "        'not glide': 'Angry', 'not gliding': 'Angry', 'not slide': 'Angry', 'not sliding': 'Angry',\n",
        "        'not slip': 'Angry', 'not slipping': 'Angry', 'not skid': 'Angry'\n",
        "}\n",
        "\n",
        "shortcuts = {\n",
        "    'u': 'you', 'y': 'why', 'r': 'are', 'doin': 'doing', 'hw': 'how', 'k': 'okay', 'm': 'am',\n",
        "    'b4': 'before',\n",
        "                   'idc': \"i do not care\", 'ty': 'thank you', 'wlcm': 'welcome', 'bc': 'because', '<3': 'love',\n",
        "                   'xoxo': 'love',\n",
        "                   'ttyl': 'talk to you later', 'gr8': 'great', 'bday': 'birthday', 'awsm': 'awesome', 'gud': 'good',\n",
        "                   'h8': 'hate',\n",
        "                   'lv': 'love', 'dm': 'direct message', 'rt': 'retweet', 'wtf': 'hate', 'idgaf': 'hate',\n",
        "                   'irl': 'in real life', 'yolo': 'you only live once', \"don't\": \"do not\", 'g8': 'great',\n",
        "                   \"won't\": \"will not\", 'tbh': 'to be honest', 'caj': 'casual', 'Ikr': 'I know, right?',\n",
        "                   'omw': 'on my way',\n",
        "                   'ofc': 'of course', 'Idc': \"I don't care\", 'Irl': 'In real life', 'tbf': 'To be fair',\n",
        "                   'obvs': 'obviously', 'v': 'very', 'atm': 'at the moment',\n",
        "                   'col': 'crying out loud', 'gbu': 'god bless you', 'gby': 'god bless you', 'gotcha': 'I got you',\n",
        "                   'hehe': 'laughing', 'haha': 'laughing', 'hf': 'have fun',\n",
        "                   'hry': 'hurry', 'hw': 'hardwork', 'idc': 'i don’t care', 'ikr': 'i know right', 'k': 'ok',\n",
        "                   'lmao': 'laughing my ass off', 'lol': 'laughing out loud',\n",
        "                   'n1': 'nice one', 'na': 'not available', 'qt': 'cutie', 'qtpi': 'cutie pie',\n",
        "                   'rip': 'rest in peace',\n",
        "                   'sry': 'sorry', 'tc': 'take care',\n",
        "                   'thnks': 'thanks', 'thx': 'thanks', 'thnk': 'thanks', 'ttyl': 'talk to you later', 'txt': 'text',\n",
        "                   'ugh': 'disgusted', 'w8': 'wait', \"not sad\": \"happy\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "U3FcfDCgjd0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##WE ARE IMPORTING THE DATASET\n",
        "\n",
        "dataset = pd.read_csv(\"/content/text2emotion.csv\", header = None)\n",
        "X = dataset[0].values ##selects the first column of the dataset and it has text data\n",
        "y = dataset[1].values ##selects the second column of the dataset and it has sentiment(emotion)"
      ],
      "metadata": {
        "id": "BpObPXS8jj_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##HERE WE ARE LABELLING ALL THE EMOTIONS\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder ## encode categorical labels into numerical label\n",
        "l = LabelEncoder() ## perform label encoding\n",
        "y = l.fit_transform(y) ##fits the encoder to the label(y) and then transforms them into numerical label\n",
        "emotions = l.inverse_transform([0,1,2,3,4,5]) ##numerical labels back to categorical values\n",
        "print(len(y))\n",
        "print(emotions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUuG1xILjqEu",
        "outputId": "82baf586-ebcf-4ee2-8db4-fa668de9e76f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19999\n",
            "['anger' 'fear' 'joy' 'love' 'sadness' 'surprise']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##NOW WE WILL CLEAN THE TEXT\n",
        "\n",
        "def remove_contradictions(text):\n",
        "    if \"n't\" in text:\n",
        "        text = text.replace(\"n't\", \" not\") ##If n't is found then  it will replace it with not\n",
        "    for pattern, replacement in contractions.items():\n",
        "        text = re.sub(pattern, replacement, text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "zV9FpNLcjtKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def removing_not(text):\n",
        "        f = re.findall(\"not\\s\\w+\", text) ## look for all the not in the sentence\n",
        "        for i in f:\n",
        "            try:\n",
        "                text = text.replace(i, nots[i]) ##replace all the nots just like for not happy it will replace as sad\n",
        "            except:\n",
        "                pass\n",
        "        text = text.lower()\n",
        "        return text"
      ],
      "metadata": {
        "id": "IH0d-PXnjxAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## FOLLOWING FUNCTION WILL REPLACE ALL THE SHORTCCUTS WITH IT ORIGINALL WORDS\n",
        "def removing_shortcuts(text):\n",
        "    full_words = []\n",
        "\n",
        "    for token in text:\n",
        "        if token in shortcuts.keys():\n",
        "            token = shortcuts[token]\n",
        "        full_words.append(token)\n",
        "    text = \" \".join(full_words)\n",
        "    return text"
      ],
      "metadata": {
        "id": "7UFoA9YEj5bH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## IT WILL RETURN THE WORD IF NOT FOUND IN STOPWORDS\n",
        "def removing_stopwords(text):\n",
        "    return [word for word in text if not word in stop_words]"
      ],
      "metadata": {
        "id": "r1HjEWw5kAur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## REMOVING WEBSITES LINKS STOPWORDS DIGITS\n",
        "corpus = []\n",
        "for i in X:\n",
        "  review = re.sub(\"[^a-zA-Z]\",\" \",i)\n",
        "  review = review.lower()\n",
        "  review = re.sub(r'http\\S+|www.\\S+', '', review)\n",
        "  review = remove_contradictions(review)\n",
        "  review = removing_not(review)\n",
        "  review = review.split()\n",
        "  review = removing_shortcuts(review)\n",
        "  review = ' '.join([i for i in review.split() if not i.isdigit()])\n",
        "  review = word_tokenize(review)\n",
        "  review = removing_stopwords(review)\n",
        "  lemma = WordNetLemmatizer()\n",
        "  review = [lemma.lemmatize(word) for word in review]\n",
        "  review = \" \".join(review)\n",
        "  corpus.append(review)"
      ],
      "metadata": {
        "id": "30UkVHskkGuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## CREATING BAG OF WORDS\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features=2000)\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "print(len(X),len(X[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hp9LvInVkM5s",
        "outputId": "1ad91c7a-b20a-401c-9ec3-05f912b2db86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19999 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TRAINING TESTING\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n"
      ],
      "metadata": {
        "id": "UtF1IM8ikRtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## GB MODEL\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "GB_classifier = GradientBoostingClassifier()\n",
        "GB_classifier.fit(X_train, y_train)\n",
        "y_pred = GB_classifier.predict(X_test)\n",
        "print(f\"Accuracy = {accuracy_score(y_test, y_pred)*100:2.4f}%\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=emotions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31P-LXzYkVIy",
        "outputId": "bcdb3829-2562-48bf-c325-24ac5f0761e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 84.6250%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.94      0.76      0.84       531\n",
            "        fear       0.95      0.71      0.81       496\n",
            "         joy       0.76      0.95      0.85      1372\n",
            "        love       0.84      0.75      0.80       323\n",
            "     sadness       0.94      0.85      0.89      1131\n",
            "    surprise       0.68      0.85      0.75       147\n",
            "\n",
            "    accuracy                           0.85      4000\n",
            "   macro avg       0.85      0.81      0.82      4000\n",
            "weighted avg       0.86      0.85      0.85      4000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CATBOOST MODEL\n",
        "\n",
        "from catboost import CatBoostClassifier ## catboost may produce a lot of loging output by default thats why we have used logging level\n",
        "CB_classifier= CatBoostClassifier(logging_level='Silent') ## setting logging level parameter to silent because we dont want to make excessive log files , so it will reduce the noise and train model faster\n",
        "CB_classifier.fit(X_train, y_train)\n",
        "y_pred = CB_classifier.predict(X_test)\n",
        "print(f\"Accuracy = {accuracy_score(y_test, y_pred)*100:2.4f}%\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=emotions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxXaP2jrdsIM",
        "outputId": "00ca3617-6f9c-4d40-bd7c-6c45d66dd64f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 89.8000%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.89      0.89      0.89       531\n",
            "        fear       0.90      0.85      0.87       496\n",
            "         joy       0.88      0.95      0.91      1372\n",
            "        love       0.88      0.72      0.79       323\n",
            "     sadness       0.95      0.93      0.94      1131\n",
            "    surprise       0.76      0.73      0.75       147\n",
            "\n",
            "    accuracy                           0.90      4000\n",
            "   macro avg       0.88      0.85      0.86      4000\n",
            "weighted avg       0.90      0.90      0.90      4000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LightGBM\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "LGBM_classifier = LGBMClassifier()\n",
        "LGBM_classifier.fit(X_train, y_train)\n",
        "y_pred = LGBM_classifier.predict(X_test)\n",
        "print(f\"Accuracy = {accuracy_score(y_test, y_pred)*100:2.4f}%\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=emotions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXxdyEctQbUE",
        "outputId": "c93e71ea-713b-4142-ddf2-470087e28e96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.208916 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3175\n",
            "[LightGBM] [Info] Number of data points in the train set: 15999, number of used features: 1190\n",
            "[LightGBM] [Info] Start training from score -1.994119\n",
            "[LightGBM] [Info] Start training from score -2.142851\n",
            "[LightGBM] [Info] Start training from score -1.088352\n",
            "[LightGBM] [Info] Start training from score -2.496411\n",
            "[LightGBM] [Info] Start training from score -1.232224\n",
            "[LightGBM] [Info] Start training from score -3.331143\n",
            "Accuracy = 88.1250%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.90      0.85      0.87       531\n",
            "        fear       0.87      0.82      0.85       496\n",
            "         joy       0.87      0.92      0.89      1372\n",
            "        love       0.77      0.84      0.80       323\n",
            "     sadness       0.96      0.91      0.93      1131\n",
            "    surprise       0.75      0.72      0.73       147\n",
            "\n",
            "    accuracy                           0.88      4000\n",
            "   macro avg       0.85      0.84      0.85      4000\n",
            "weighted avg       0.88      0.88      0.88      4000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#decision tree model\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "decision_classifier = DecisionTreeClassifier()\n",
        "decision_classifier.fit(X_train, y_train)\n",
        "y_pred = decision_classifier.predict(X_test)\n",
        "print(f\"Accuracy = {accuracy_score(y_test, y_pred)*100:2.4f}%\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=emotions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBQjkdI_RnYU",
        "outputId": "cd0258ba-454b-4523-eaca-475dc5fb221a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 83.5250%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.78      0.86      0.82       531\n",
            "        fear       0.79      0.83      0.81       496\n",
            "         joy       0.89      0.80      0.84      1372\n",
            "        love       0.72      0.76      0.74       323\n",
            "     sadness       0.88      0.90      0.89      1131\n",
            "    surprise       0.66      0.77      0.71       147\n",
            "\n",
            "    accuracy                           0.84      4000\n",
            "   macro avg       0.79      0.82      0.80      4000\n",
            "weighted avg       0.84      0.84      0.84      4000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#kernal SVM\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "kernal_classifier = SVC(kernel='rbf')\n",
        "kernal_classifier.fit(X_train, y_train)\n",
        "y_pred = kernal_classifier.predict(X_test)\n",
        "print(f\"Accuracy = {accuracy_score(y_test, y_pred)*100:2.4f}%\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=emotions))"
      ],
      "metadata": {
        "id": "vtRQ3d85SrIG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a19384d-9532-42df-8a1f-0fcda05eb3e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 84.3750%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.89      0.75      0.81       531\n",
            "        fear       0.88      0.72      0.79       496\n",
            "         joy       0.79      0.96      0.86      1372\n",
            "        love       0.89      0.58      0.70       323\n",
            "     sadness       0.88      0.92      0.90      1131\n",
            "    surprise       0.93      0.54      0.68       147\n",
            "\n",
            "    accuracy                           0.84      4000\n",
            "   macro avg       0.88      0.74      0.79      4000\n",
            "weighted avg       0.85      0.84      0.84      4000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#VOTING CLASSIFIER\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "GB_classifier = GradientBoostingClassifier()\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "LGBM_classifier = LGBMClassifier()\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "decision_classifier = DecisionTreeClassifier()\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "kernal_classifier = SVC(kernel = 'rbf')\n",
        "\n",
        "##from catboost import CatBoostClassifier\n",
        "##CB_classifier= CatBoostClassifier(logging_level='Silent')\n",
        "\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('GBC', GB_classifier),\n",
        "        ('LGBMC', LGBM_classifier),\n",
        "        ('DTC', decision_classifier),\n",
        "        ('KSVMC', kernal_classifier)\n",
        "       ## ('CBC',CB_classifier)\n",
        "    ],\n",
        "    voting='hard')##The voting parameter is set to 'hard', indicating that the final prediction of the ensemble will be based on a majority vote among the classifiers\n",
        "ensemble.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "ZgSwopqGQ4uS",
        "outputId": "cc1b1521-e759-4ad9-eaf1-bccc7df97a09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223091 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3175\n",
            "[LightGBM] [Info] Number of data points in the train set: 15999, number of used features: 1190\n",
            "[LightGBM] [Info] Start training from score -1.994119\n",
            "[LightGBM] [Info] Start training from score -2.142851\n",
            "[LightGBM] [Info] Start training from score -1.088352\n",
            "[LightGBM] [Info] Start training from score -2.496411\n",
            "[LightGBM] [Info] Start training from score -1.232224\n",
            "[LightGBM] [Info] Start training from score -3.331143\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VotingClassifier(estimators=[('GBC', GradientBoostingClassifier()),\n",
              "                             ('LGBMC', LGBMClassifier()),\n",
              "                             ('DTC', DecisionTreeClassifier()),\n",
              "                             ('KSVMC', SVC())])"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;GBC&#x27;, GradientBoostingClassifier()),\n",
              "                             (&#x27;LGBMC&#x27;, LGBMClassifier()),\n",
              "                             (&#x27;DTC&#x27;, DecisionTreeClassifier()),\n",
              "                             (&#x27;KSVMC&#x27;, SVC())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;GBC&#x27;, GradientBoostingClassifier()),\n",
              "                             (&#x27;LGBMC&#x27;, LGBMClassifier()),\n",
              "                             (&#x27;DTC&#x27;, DecisionTreeClassifier()),\n",
              "                             (&#x27;KSVMC&#x27;, SVC())])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>GBC</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>LGBMC</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>DTC</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>KSVMC</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = ensemble.predict(X_test)\n",
        "print(f\"Accuracy = {accuracy_score(y_test, y_pred)*100:2.4f}%\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=emotions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYIB7Ht-Tdrb",
        "outputId": "4a01d262-c4f0-4617-e1ef-51d952963b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 88.2250%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.89      0.85      0.87       531\n",
            "        fear       0.89      0.84      0.86       496\n",
            "         joy       0.84      0.96      0.90      1372\n",
            "        love       0.89      0.70      0.78       323\n",
            "     sadness       0.95      0.89      0.92      1131\n",
            "    surprise       0.78      0.72      0.75       147\n",
            "\n",
            "    accuracy                           0.88      4000\n",
            "   macro avg       0.87      0.83      0.85      4000\n",
            "weighted avg       0.89      0.88      0.88      4000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##predict the result\n",
        "\n",
        "\n",
        "def predict_emotion(text, classifier):\n",
        "  text = text.replace(\"\\n\",\" \")\n",
        "  review = re.sub(\"[^a-zA-Z]\",\" \",text)## regular expression to remove any characters that are not alphabetic\n",
        "  review = review.lower()\n",
        "  review = re.sub(r'http\\S+|www.\\S+', '', review)\n",
        "  review = remove_contradictions(review)\n",
        "  review = removing_not(review)\n",
        "  review = review.split()\n",
        "  review = removing_shortcuts(review)\n",
        "  review = ' '.join([i for i in review.split() if not i.isdigit()])##removes digits from the text\n",
        "  review = word_tokenize(review)\n",
        "  review = removing_stopwords(review)\n",
        "  lemma = WordNetLemmatizer()\n",
        "  review = [lemma.lemmatize(word) for word in review]\n",
        "  review = \" \".join(review)\n",
        "  test = cv.transform([review]).toarray()\n",
        "  result = classifier.predict(test)\n",
        "  result = l.inverse_transform(result)\n",
        "  return result"
      ],
      "metadata": {
        "id": "FESI9inQZbm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=''' Hrutuja was feeling low'''\n",
        "\n",
        "print(predict_emotion(text, ensemble))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQcT4yVu_STy",
        "outputId": "70f44ce8-7c24-4ecd-98ff-cc4788848961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sadness']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=''' hrutuja was not able to do connection so she is feeling low'''\n",
        "print(predict_emotion(text, ensemble))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgD15IZH_6JF",
        "outputId": "baa5cab7-2ca9-4507-f408-5762d25eb77f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sadness']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=''' ashwini was too excited as she has taken the autograph of M.S Dhoni'''\n",
        "print(predict_emotion(text, ensemble))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjLON98dAsYt",
        "outputId": "c372a704-9ea9-469c-a7e8-9e0f8478aad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['joy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uO5_ubwYHM3C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}